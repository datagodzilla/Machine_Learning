{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- http://people.duke.edu/~rnau/regintro.htm\n",
    "- https://github.com/justmarkham/DAT5/blob/master/notebooks/09_linear_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression Pros and Cons\n",
    "\n",
    "**Pros:** \n",
    "- Fast\n",
    "- No tuning required\n",
    "- Highly interpretable\n",
    "- Well-understood\n",
    "\n",
    "**Cons:** \n",
    "- Unlikely to produce the best predictive accuracy\n",
    "    - Presumes a linear relationship between the features and response\n",
    "    - If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Equation of line:\n",
    "\n",
    "$y = w_1x_1 + w_2$\n",
    "\n",
    "where,  \n",
    "slope: $w_1$  \n",
    "y-intercept: $w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "The objective is to minimize the error between the point and estimated line.\n",
    "\n",
    "**Error function:**\n",
    "\n",
    "$w_i \\to w_i - \\alpha \\frac{\\partial }{\\partial w_i} Error $\n",
    "\n",
    "- The plot of the error function is a parabola with weights on x-axis and Error on the y-axis.\n",
    "- Way to descend from large error is to take the derivative or gradient of the Error function wrt the weights $w_i$.\n",
    "- The gradient points to the direction where the function increases the most. Therefore, the negative of this gradient is going to point down to the direction where the function decreases the most.\n",
    "- The error descends in the direction of the negative gradient. This means we change the weights $w_i$ to $w_i - \\alpha \\frac{\\partial }{\\partial w_i} Error $.\n",
    "- By multiplying the derivative of the Error term wrt the weights with learning rate $(\\alpha)$ the step-size is small.\n",
    "- This means that the error function is decreasing and we're closer to the minimum.\n",
    "- By doing this many times, the result is either a minimum or a pretty good value where the error is small which signifies a pretty good solution to the linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Functions\n",
    "The two most common error functions for linear regression are:\n",
    "- Mean Absolute Error (MAE)\n",
    "2. Mean Squared Error (MSE)\n",
    "\n",
    "##### Mean Absolute Error:\n",
    "- The vertical distance from the point to the line is the $y - \\hat{y}.$\n",
    "\n",
    "Mean Absolute Error is the sum of all the absolute errors divided by the number of points:  \n",
    "\n",
    "$Error = \\frac{1}{m} \\sum_{i=1}^{m} |y - \\hat{y}|$\n",
    "\n",
    "Using gradient descent we get the best possible fit line with the smallest possible MAE.\n",
    "\n",
    "##### Mean Squared Error:\n",
    "\n",
    "Mean Squared Error is the sum of all the squared errors divided by the number of points:  \n",
    "\n",
    "$Error = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2$\n",
    "\n",
    "By minimizing the average sum of squared errors, MSE is minimized and we get the best possible fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country  Life expectancy       BMI\n",
      "0  Afghanistan             52.8  20.62058\n",
      "1      Albania             76.8  26.44657\n",
      "2      Algeria             75.5  24.59620\n",
      "3      Andorra             84.6  27.63048\n",
      "4       Angola             56.7  22.25083\n",
      "Life expenctancy: [[60.31564716]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add import statements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "# TODO: Load the data\n",
    "bmi_life_data = pd.read_csv('bmi_and_life_expectancy.csv')\n",
    "print(bmi_life_data.head())\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#TODO: Fit the model and Assign it to bmi_life_model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Make a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict([[21.07931]])\n",
    "print(\"Life expenctancy: {}\".format(laos_life_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + \\beta_3 \\times x_3$\n",
    "\n",
    "\n",
    "$\\hat{y} = w_1x_1 + w_2x_2 + ... + w_{n-1}x_{n-1} + w_n$\n",
    "\n",
    "$y = w_1x_1 + w_2x_2 + ... + w_{n}x_{n} + b$\n",
    "\n",
    "- $y$ is the response\n",
    "- $w_n$ is the intercept\n",
    "- $w_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $w_{n-1}$ is the coefficient for $x_{n-1}$ (the (n-1)th feature)\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**\n",
    "- These values are \"learned\" during the model fitting step using the \"least squares\" criterion\n",
    "- Then, the fitted model can be used to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "The model equation with *n* predictor variables:\n",
    "\n",
    "$y = w_1x_1 + w_2x_2 + ... + w_{n}x_{n} + b$\n",
    "\n",
    "- $y$ is the response\n",
    "- $w_n$ is the intercept\n",
    "- $w_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $w_{n-1}$ is the coefficient for $x_{n-1}$ (the (n-1)th feature)\n",
    "\n",
    "In this case:\n",
    "\n",
    "The $w$ values are called the **model coefficients**\n",
    "- These values are \"learned\" during the model fitting step using the \"least squares\" criterion\n",
    "- Then, the fitted model can be used to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "**Data:**   \n",
    "$x_1, x_2, ...., x_m$  \n",
    "\n",
    "**Labels:**  \n",
    "$y_1, y_2, ...., y_m$   \n",
    "\n",
    "**Predictions:**   \n",
    "$\\hat{y_i} = w_1x_i + w2$  \n",
    "\n",
    "where,    \n",
    "*slope:*  $w_1$   \n",
    "*y_intercept:* $w_2$    \n",
    "\n",
    "**Mean Squared Error:**\n",
    "\n",
    "Given the values  of $w_1$ and $w_2$, we can calculate the predictions and the error based on these values of $w_1$ and $w_2$.\n",
    "\n",
    "$Error(w_1, w_2) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 $  \n",
    "\n",
    "In order to minimize this error, we need to take the derivatives wrt the input variables $w_1$ and $w_2$ and set them both equal to zero. We calculate the derivites and we get these two formulas:\n",
    "\n",
    "$0 = \\frac{\\sum{x_i}^2}{m}w_1 + \\frac{\\sum{x_i}}{m}w_2 + \\frac{\\sum{x_iy_i}}{m} $   \n",
    "\n",
    "$0 = \\frac{\\sum{x_i}}{m}w_1 + w_2 + \\frac{\\sum{y_i}}{m} $   \n",
    "\n",
    "Now, we need to solve for $w_1$ and $w_2$ for these 2 equations to be zero. We have a system of two equations and two unknowns to be solved.\n",
    "\n",
    "For a system with greater number of dimensions in inputs, the problem would have *n* equations with *n* unknowns which will need a lot of computational power depending on the size of *n*.\n",
    "\n",
    "Therefore, gradient decent method is used to obtain a solution that fits our data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Dimensional solution\n",
    "\n",
    "**Data:**   \n",
    "$x_1, x_2, ...., x_m$  \n",
    "\n",
    "**Labels:**  \n",
    "$y_1, y_2, ...., y_m$   \n",
    "\n",
    "**Predictions:**   \n",
    "$\\hat{y_i} = w_1x_i + w_2$  \n",
    "\n",
    "where,    \n",
    "*slope:*  $w_1$   \n",
    "*y_intercept:* $w_2$    \n",
    "\n",
    "**Mean Squared Error:**\n",
    "\n",
    "Given the values  of $w_1$ and $w_2$, we can calculate the predictions and the error based on these values of $w_1$ and $w_2$.\n",
    "\n",
    "$E(w_1, w_2) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 $  \n",
    "\n",
    "We need to minimize this error function. Ignoring $\\frac{1}{m}$, and replacing the value of $\\hat{y_i} = w_1x_i + w_2$, we get:\n",
    "\n",
    "$E(w_1, w_2) = \\sum_{i=1}^{m} (\\hat{y} - y)^2      \n",
    "             = \\sum_{i=1}^{m} (w_1x_i + w_2 - y)^2  $    \n",
    "\n",
    "In order to minimize this error function, we need to take the derivatives wrt $w_1$ and $w_2$ and set them equal to 0.\n",
    "\n",
    "Using the chain rule, we get:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_1} = \\sum_{i=1}^{m} (w_1x_i + w_2 - y_i)x_i = w_1 \\sum_{i=1}^{m} {x_i}^2 + w_2 \\sum_{i=1}^{m} x_i - \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_2} = \\sum_{i=1}^{m} (w_1x_i + w_2 - y_i) = w_1 \\sum_{i=1}^{m} x_i + w_2 \\sum_{i=1}^{m} 1 - \\sum_{i=1}^{m} y_i $\n",
    "\n",
    "Setting the two equations to zero gives us the following two equations and two variables (where the variables are $w_1$ and $w_2$)     \n",
    "\n",
    "$0 = w_1 \\sum_{i=1}^{m} {x_i}^2 + w_2 \\sum_{i=1}^{m} x_i - \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "$0 = w_1 \\sum_{i=1}^{m} x_i + w_2 \\sum_{i=1}^{m} 1 - \\sum_{i=1}^{m} y_i $ \n",
    "\n",
    "then\n",
    "\n",
    "$ w_1(\\sum_{i=1}^{m} {x_i}^2) + w_2(\\sum_{i=1}^{m} x_i) = \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "$ w_1(\\sum_{i=1}^{m} x_i) + w_2(m) = \\sum_{i=1}^{m} y_i $ \n",
    "\n",
    "We can solve the 2 equations and 2 variables by multiplying the first equation by $ w_1(\\sum_{i=1}^{m} x_i) $ and the second equation by *m* and subtract them to obtain a value for $w_1$, and then replace this value in the first equation, we get the following:\n",
    "\n",
    "$ w_1 = \\frac{m(\\sum_{i=1}^{m} x_iy_i) - (\\sum_{i=1}^{m} x_i)(\\sum_{i=1}^{m} y_i)}{m(\\sum_{i=1}^{m} {x_i}^2) - (\\sum_{i=1}^{m} x_i)^2}$\n",
    "\n",
    "\n",
    "$ w_2 = \\frac{(\\sum_{i=1}^{m} x_iy_i)(\\sum_{i=1}^{m} x_i)  -  (\\sum_{i=1}^{m} {x_i}^2)(\\sum_{i=1}^{m} y_i)}{m(\\sum_{i=1}^{m} x_i)^2 - m(\\sum_{i=1}^{m} {x_i}^2)}$      \n",
    "\n",
    "These are the values for the model coefficients $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-Dimensional solution    \n",
    "Instead of 2 dimensions as discussed above, if the data has *n* dimensions, the procedure involves matrix operations. The matrix X contains data where each row is one of the data points, and ${x_0}^{(i)} = 1$ represents bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](X.gif \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let *y* be the matrix containing labels. \n",
    "\n",
    "![alt text](y.gif \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let *W* be the matrix containing weights. \n",
    "\n",
    "![alt text](W.gif \"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the equation for the mean square error can be written as the following matrix product:\n",
    "\n",
    "$E(W) = \\frac{1}{m}((XW)^T - y^T)(XW -y) $\n",
    "\n",
    "Again, since we need to minimize it, we can forget about the factor of $\\frac{1}{m}$, so expanding, we get    \n",
    "\n",
    "$E(W) = W^{T}X^{T}XW - (XW)^{T}y - y^{T}(XW) + y^{T}y.$\n",
    "\n",
    "Notice that in the sum, the second and the third terms are the same, since it's the inner product of two vectors, which means it's the sum of the products of its coordinates. Therefore,\n",
    "\n",
    "$ E(W) =  W^{T}X^{T}XW - 2(XW)^{T}y + y^{T}y.$\n",
    "\n",
    "Now, to minimize this, we need to take the derivative with respect to all values in the matrix *W*. Using the chain rule, as we used above, we get the following:\n",
    "\n",
    "And in order to set this equal to zero, we need\n",
    "\n",
    "$X^{T}XW - X^{T}y = 0 $, or equivalently,     \n",
    "\n",
    "$W = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "This are the required weights.\n",
    "\n",
    "This method is computationally intensive, since finding the inverse of the matrix $X^{T}X$ is hard, if the value of *n* is large. Alternatively, gradient descent is repeated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X values:\n",
      " [[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      "  4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]]\n",
      "\n",
      "Y values:\n",
      " [24.]\n",
      "\n",
      "Prediction:\n",
      " [23.68284712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset\n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "print('X values:\\n {}'.format(x[:1]))\n",
    "print()\n",
    "print('Y values:\\n {}'.format(y[:1]))\n",
    "print()\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x,y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "\n",
    "# Predict housing price for the sample_house\n",
    "prediction = lr_model.predict(sample_house)\n",
    "print('Prediction:\\n {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
