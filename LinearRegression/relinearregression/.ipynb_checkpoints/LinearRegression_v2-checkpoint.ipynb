{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Equation of line:\n",
    "\n",
    "$y = w_1x_1 + w_2$\n",
    "\n",
    "where,  \n",
    "slope: $w_1$  \n",
    "y-intercept: $w_2$\n",
    "\n",
    "![alt text](LR.png \"LinearRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Functions\n",
    "The two most common error functions for linear regression are:\n",
    "- Mean Absolute Error (MAE)\n",
    "2. Mean Squared Error (MSE)\n",
    "\n",
    "##### Mean Absolute Error:\n",
    "- The vertical distance from the point to the line is the $y - \\hat{y}.$\n",
    "\n",
    "Mean Absolute Error is the sum of all the absolute errors divided by the number of points:  \n",
    "\n",
    "$Error = \\frac{1}{m} \\sum_{i=1}^{m} |y - \\hat{y}|$\n",
    "\n",
    "Using gradient descent we get the best possible fit line with the smallest possible MAE.\n",
    "\n",
    "##### Mean Squared Error:\n",
    "\n",
    "Mean Squared Error is the sum of all the squared errors divided by the number of points:  \n",
    "\n",
    "$Error = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2$\n",
    "\n",
    "By minimizing the average sum of squared errors, MSE is minimized and we get the best possible fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "**Data:**   \n",
    "$x_1, x_2, ...., x_m$  \n",
    "\n",
    "**Labels:**  \n",
    "$y_1, y_2, ...., y_m$   \n",
    "\n",
    "**Predictions:**   \n",
    "$\\hat{y_i} = w_1x_i + w2$  \n",
    "\n",
    "where,    \n",
    "*slope:*  $w_1$   \n",
    "*y_intercept:* $w_2$    \n",
    "\n",
    "**Mean Squared Error:**\n",
    "\n",
    "Given the values  of $w_1$ and $w_2$, we can calculate the predictions and the error based on these values of $w_1$ and $w_2$.\n",
    "\n",
    "$Error(w_1, w_2) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 $  \n",
    "\n",
    "In order to minimize this error, we need to take the derivatives wrt the input variables $w_1$ and $w_2$ and set them both equal to zero. We calculate the derivites and we get these two formulas:\n",
    "\n",
    "$0 = \\frac{\\sum{x_i}^2}{m}w_1 + \\frac{\\sum{x_i}}{m}w_2 + \\frac{\\sum{x_iy_i}}{m} $   \n",
    "\n",
    "$0 = \\frac{\\sum{x_i}}{m}w_1 + w_2 + \\frac{\\sum{y_i}}{m} $   \n",
    "\n",
    "Now, we need to solve for $w_1$ and $w_2$ for these 2 equations to be zero. We have a system of two equations and two unknowns to be solved.\n",
    "\n",
    "For a system with greater number of dimensions in inputs, the problem would have *n* equations with *n* unknowns which will need a lot of computational power depending on the size of *n*.\n",
    "\n",
    "Therefore, gradient decent method is used to obtain a solution that fits our data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Dimensional solution\n",
    "\n",
    "**Data:**   \n",
    "$x_1, x_2, ...., x_m$  \n",
    "\n",
    "**Labels:**  \n",
    "$y_1, y_2, ...., y_m$   \n",
    "\n",
    "**Predictions:**   \n",
    "$\\hat{y_i} = w_1x_i + w_2$  \n",
    "\n",
    "where,    \n",
    "*slope:*  $w_1$   \n",
    "*y_intercept:* $w_2$    \n",
    "\n",
    "**Mean Squared Error:**\n",
    "\n",
    "Given the values  of $w_1$ and $w_2$, we can calculate the predictions and the error based on these values of $w_1$ and $w_2$.\n",
    "\n",
    "$E(w_1, w_2) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y} - y)^2 $  \n",
    "\n",
    "We need to minimize this error function. Ignoring $\\frac{1}{m}$, and replacing the value of $\\hat{y_i} = w_1x_i + w_2$, we get:\n",
    "\n",
    "$E(w_1, w_2) = \\sum_{i=1}^{m} (\\hat{y} - y)^2      \n",
    "             = \\sum_{i=1}^{m} (w_1x_i + w_2 - y)^2  $    \n",
    "\n",
    "In order to minimize this error function, we need to take the derivatives wrt $w_1$ and $w_2$ and set them equal to 0.\n",
    "\n",
    "Using the chain rule, we get:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_1} = \\sum_{i=1}^{m} (w_1x_i + w_2 - y_i)x_i = w_1 \\sum_{i=1}^{m} {x_i}^2 + w_2 \\sum_{i=1}^{m} x_i - \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_2} = \\sum_{i=1}^{m} (w_1x_i + w_2 - y_i) = w_1 \\sum_{i=1}^{m} x_i + w_2 \\sum_{i=1}^{m} 1 - \\sum_{i=1}^{m} y_i $\n",
    "\n",
    "Setting the two equations to zero gives us the following two equations and two variables (where the variables are $w_1$ and $w_2$)     \n",
    "\n",
    "$0 = w_1 \\sum_{i=1}^{m} {x_i}^2 + w_2 \\sum_{i=1}^{m} x_i - \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "$0 = w_1 \\sum_{i=1}^{m} x_i + w_2 \\sum_{i=1}^{m} 1 - \\sum_{i=1}^{m} y_i $ \n",
    "\n",
    "then\n",
    "\n",
    "$ w_1(\\sum_{i=1}^{m} {x_i}^2) + w_2(\\sum_{i=1}^{m} x_i) = \\sum_{i=1}^{m} x_iy_i $\n",
    "\n",
    "$ w_1(\\sum_{i=1}^{m} x_i) + w_2(m) = \\sum_{i=1}^{m} y_i $ \n",
    "\n",
    "We can solve the 2 equations and 2 variables by multiplying the first equation by $ w_1(\\sum_{i=1}^{m} x_i) $ and the second equation by *m* and subtract them to obtain a value for $w_1$, and then replace this value in the first equation, we get the following:\n",
    "\n",
    "$ w_1 = \\frac{m(\\sum_{i=1}^{m} x_iy_i) - (\\sum_{i=1}^{m} x_i)(\\sum_{i=1}^{m} y_i)}{m(\\sum_{i=1}^{m} {x_i}^2) - (\\sum_{i=1}^{m} x_i)^2}$\n",
    "\n",
    "\n",
    "$ w_2 = \\frac{(\\sum_{i=1}^{m} x_iy_i)(\\sum_{i=1}^{m} x_i)  -  (\\sum_{i=1}^{m} {x_i}^2)(\\sum_{i=1}^{m} y_i)}{m(\\sum_{i=1}^{m} x_i)^2 - m(\\sum_{i=1}^{m} {x_i}^2)}$      \n",
    "\n",
    "These are the values for the model coefficients $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "The model equation with *n* predictor variables:\n",
    "\n",
    "$y = x_0 + w_1x_1 + w_2x_2  + ... +  w_{n}x_{n}$\n",
    "\n",
    "- $y$ is the response\n",
    "- $w_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $w_n$ is the coefficient for $x_n$ (the *n*th feature)\n",
    "\n",
    "In this case:\n",
    "\n",
    "The $w$ values are called the **model coefficients**\n",
    "- These values are \"learned\" during the model fitting step using the \"least squares\" criterion\n",
    "- Then, the fitted model can be used to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-Dimensional solution    \n",
    "Instead of 2 dimensions as discussed above, if the data has *n* dimensions, the procedure involves matrix operations. The matrix X contains data where each row is one of the data points, and ${x_0}^{(i)} = 1$ represents bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](X.gif \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let *y* be the matrix containing labels. \n",
    "\n",
    "![alt text](y.gif \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let *W* be the matrix containing weights. \n",
    "\n",
    "![alt text](W.gif \"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the equation for the **Mean Squared Error** can be written as the following matrix product:\n",
    "\n",
    "$E(W) = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2$   \n",
    "\n",
    "If $e(w) = (y-\\hat{y})$ and $\\hat{y} = XW $, in terms of multiplication, $e^2$ can be written as $e^{T}e$    \n",
    "\n",
    "$E(W) = \\frac{1}{2m} (y - XW){^T}(y-XW) $\n",
    "\n",
    "$E(W) = \\frac{1}{2m}((XW)^T - y^T)(XW -y) $\n",
    "\n",
    "Again, since we need to minimize it, we can forget about the factor of $\\frac{1}{2m}$, so expanding, we get    \n",
    "\n",
    "$E(W) = W^{T}X^{T}XW - (XW)^{T}y - y^{T}(XW) + y^{T}y.$\n",
    "\n",
    "Notice that in the sum, the second and the third terms are the same, since it's the inner product of two vectors, which means it's the sum of the products of its coordinates. Therefore,\n",
    "\n",
    "$ E(W) =  W^{T}X^{T}XW - 2(XW)^{T}y + y^{T}y.$\n",
    "\n",
    "Now, to minimize this, we need to take the derivative with respect to all values in the matrix *W*. Using the chain rule, as we used above, we get the following:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial W} = 2X^{T}XW - 2X^{T}y = 0 $      \n",
    "\n",
    "And in order to set this equal to zero, we need\n",
    "\n",
    "$X^{T}XW - X^{T}y = 0 $, or equivalently,     \n",
    "\n",
    "$W = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "This are the required weights.\n",
    "\n",
    "This method is computationally intensive, since finding the inverse of the matrix $X^{T}X$ is hard, if the value of *n* is large. Alternatively, gradient descent is repeated many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "The objective is to minimize the error between the point and estimated line.\n",
    "\n",
    "![alt text](GD.png \"SLR\")\n",
    "\n",
    "**Error function:**\n",
    "\n",
    "$w_i \\to w_i - \\alpha \\frac{\\partial }{\\partial w_i} Error $\n",
    "\n",
    "- The plot of the error function is a parabola with weights on x-axis and Error on the y-axis.\n",
    "- Way to descend from large error is to take the derivative or gradient of the Error function wrt the weights $w_i$.\n",
    "- The gradient points to the direction where the function increases the most. Therefore, the negative of this gradient is going to point down to the direction where the function decreases the most.\n",
    "- The error descends in the direction of the negative gradient. This means we change the weights $w_i$ to $w_i - \\alpha \\frac{\\partial }{\\partial w_i} Error $.\n",
    "- By multiplying the derivative of the Error term wrt the weights with learning rate $(\\alpha)$ the step-size is small.\n",
    "- This means that the error function is decreasing and we're closer to the minimum.\n",
    "- By doing this many times, the result is either a minimum or a pretty good value where the error is small which signifies a pretty good solution to the linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Gradient Descent\n",
    "\n",
    "- The MAE/MSE minimization is applied to data points all at the same time and the process is repeated many times\n",
    "\n",
    "![alt text](batch.png \"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "- The MAE/MSE minimization is applied to data points one by one and the process is repeated many times\n",
    "\n",
    "![alt text](stochastic.png \"Stochastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch Gradient Descent\n",
    "\n",
    "- The MAE/MSE minimization is applied to data points into many small batches and the process is repeated many times. \n",
    "- Each batch will have roughly the same number of points and each batch is used to update the weights.\n",
    "\n",
    "![alt text](minibatch.png \"minibatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Assumptions\n",
    "- Works best when the data is linear\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Higher order polynomial equations to fit data following a non-linear trend.\n",
    "\n",
    "$ \\hat{y} = w_1x^3 + w_2x^2 + w_3x + w_4 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity of model\n",
    "\n",
    "Simpler models have a tendency to generalize better than complex models. To make complexity of model into account while measuring the error, regularization techniques are used.\n",
    "\n",
    "Total error = Error from misclassification + complexity\n",
    "\n",
    "#### L1 Regularization\n",
    "\n",
    "- Takes the coefficients of the model and adds the absolute values to the error.\n",
    "- Computationally inefficient, unless the data is sparse\n",
    "- Feature selection\n",
    "\n",
    "#### L2 Regularization\n",
    "\n",
    "- Takes the sum of squares of coefficients of the model and adds the value to the error.\n",
    "- Computationally efficient (computing derivatives of squares is efficient), good for non-sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country  Life expectancy       BMI\n",
      "0  Afghanistan             52.8  20.62058\n",
      "1      Albania             76.8  26.44657\n",
      "2      Algeria             75.5  24.59620\n",
      "3      Andorra             84.6  27.63048\n",
      "4       Angola             56.7  22.25083\n",
      "Life expenctancy: [[60.31564716]]\n"
     ]
    }
   ],
   "source": [
    "# Add import statements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "# TODO: Load the data\n",
    "bmi_life_data = pd.read_csv('bmi_and_life_expectancy.csv')\n",
    "print(bmi_life_data.head())\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#TODO: Fit the model and Assign it to bmi_life_model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Make a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict([[21.07931]])\n",
    "print(\"Life expenctancy: {}\".format(laos_life_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X values:\n",
      " [[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      "  4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]]\n",
      "\n",
      "Y values:\n",
      " [24.]\n",
      "\n",
      "Prediction:\n",
      " [23.68284712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset\n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "print('X values:\\n {}'.format(x[:1]))\n",
    "print()\n",
    "print('Y values:\\n {}'.format(y[:1]))\n",
    "print()\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x,y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "\n",
    "# Predict housing price for the sample_house\n",
    "prediction = lr_model.predict(sample_house)\n",
    "print('Prediction:\\n {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  http://facweb.cs.depaul.edu/mobasher/classes/csc478/Notes/IPython%20Notebook%20-%20Regression.html\n",
    "\n",
    "- https://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn-pandas#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "import numpy as np\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# In order to do multiple regression we need to add a column of 1s for x0\n",
    "x = np.array([np.concatenate((v,[1])) for v in boston.data])\n",
    "y = boston.target\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=120, suppress=True, edgeitems=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "linreg.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.   25.03 30.57 28.61 27.94 25.26 23.   19.54 11.52 18.92]\n"
     ]
    }
   ],
   "source": [
    "# Let's see predictions for the first 10 instances\n",
    "print(linreg.predict(x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.   3.43 4.13 4.79 8.26 3.44 0.1  7.56 4.98 0.02]\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE on training data\n",
    "# p = np.array([linreg.predict(xi) for xi in x])\n",
    "p = linreg.predict(x)\n",
    "# Now we can constuct a vector of errors\n",
    "err = abs(p-y)\n",
    "\n",
    "# Let's see the error on the first 10 predictions\n",
    "print(err[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.679191295697281\n"
     ]
    }
   ],
   "source": [
    "# Dot product of error vector with itself gives us the sum of squared errors\n",
    "total_error = np.dot(err,err)\n",
    "# Compute RMSE\n",
    "rmse_train = np.sqrt(total_error/len(p))\n",
    "print(rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Coefficients: \n",
      " [ -0.11   0.05   0.02   2.69 -17.77   3.81   0.    -1.48   0.31  -0.01  -0.95   0.01  -0.52   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# We can view the regression coefficients\n",
    "print('Regression Coefficients: \\n', linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "pl.plot(p, y,'ro')\n",
    "pl.plot([0,50],[0,50], 'g-')\n",
    "pl.xlabel('predicted')\n",
    "pl.ylabel('real')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object with a ridge coefficient 0.5\n",
    "ridge = Ridge(fit_intercept=True, alpha=0.5)\n",
    "\n",
    "# Train the model using the training set\n",
    "ridge.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Ridge Regression\n",
      "RMSE on training: 4.6854\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE on training data\n",
    "# p = np.array([ridge.predict(xi) for xi in x])\n",
    "p = ridge.predict(x)\n",
    "err = p-y\n",
    "total_error = np.dot(err,err)\n",
    "rmse_train = np.sqrt(total_error/len(p))\n",
    "\n",
    "method_name = 'Ridge Regression'\n",
    "print('Method: %s' %method_name)\n",
    "print('RMSE on training: %.4f' %rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression using Stochastic Gradeint Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Stochastic Gradient Descent Regression\n",
      "RMSE on training: 4.8071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsurampudi\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SGD is very senstitive to varying-sized feature values. So, first we need to do feature scaling.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_s = scaler.transform(x)\n",
    "\n",
    "sgdreg = SGDRegressor(penalty='l2', alpha=0.15, max_iter=200)\n",
    "\n",
    "# Compute RMSE on training data\n",
    "sgdreg.fit(x_s,y)\n",
    "p = sgdreg.predict(x_s)\n",
    "err = p-y\n",
    "total_error = np.dot(err,err)\n",
    "rmse_train = np.sqrt(total_error/len(p))\n",
    "\n",
    "method_name = 'Stochastic Gradient Descent Regression'\n",
    "print('Method: %s' %method_name)\n",
    "print('RMSE on training: %.4f' %rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
